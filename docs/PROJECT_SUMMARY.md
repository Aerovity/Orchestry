# Orchestry - Project Summary

## ğŸ¯ What is Orchestry?

Orchestry is a complete, working CLI-based Multi-Agent LLM Reinforcement Learning Environment built in Python. It trains multiple Claude AI agents to collaborate on creative tasks through reinforcement learning.

**Current Implementation**: Collaborative Story Writing
- 3 agents (Creative Writer, Editor, Narrator) work together
- Agents learn to build on each other's ideas
- Performance improves over episodes through RL

## ğŸ“ Complete Project Structure

```
Orchestry/
â”œâ”€â”€ main.py                    # âœ… CLI entry point (451 lines)
â”œâ”€â”€ config.yaml               # âœ… Configuration file
â”œâ”€â”€ requirements.txt          # âœ… Dependencies
â”œâ”€â”€ .env.example             # âœ… Environment template
â”œâ”€â”€ .gitignore               # âœ… Git ignore file
â”œâ”€â”€ README.md                # âœ… Complete documentation (350+ lines)
â”œâ”€â”€ QUICKSTART.md            # âœ… Quick start guide
â”œâ”€â”€ PROJECT_SUMMARY.md       # âœ… This file
â”œâ”€â”€ setup.py                 # âœ… Setup verification script
â”‚
â”œâ”€â”€ src/                     # âœ… Core implementation
â”‚   â”œâ”€â”€ __init__.py         # Package initialization
â”‚   â”œâ”€â”€ agent.py            # LLM Agent (260 lines)
â”‚   â”œâ”€â”€ environment.py      # RL Environment (270 lines)
â”‚   â”œâ”€â”€ rewards.py          # Reward System (350 lines)
â”‚   â”œâ”€â”€ trainer.py          # Training Loop (320 lines)
â”‚   â””â”€â”€ utils.py            # Utilities & Plotting (260 lines)
â”‚
â”œâ”€â”€ tests/                   # âœ… Test suite
â”‚   â””â”€â”€ test_basic.py       # Basic tests (180 lines)
â”‚
â””â”€â”€ runs/                    # Auto-generated during training
    â””â”€â”€ {timestamp}/
        â”œâ”€â”€ episodes.json
        â”œâ”€â”€ rewards.csv
        â”œâ”€â”€ metrics.json
        â”œâ”€â”€ agent_stats.json
        â””â”€â”€ plots/
            â”œâ”€â”€ training_curves.png
            â””â”€â”€ collaboration_analysis.png
```

**Total Lines of Code**: ~2,100+ lines of well-documented Python

## âœ… Implemented Features

### 1. **Core Agent System** ([agent.py](src/agent.py))
- âœ… LLM-powered agents with roles and goals
- âœ… Episodic and long-term memory
- âœ… Learned behaviors that evolve
- âœ… Dynamic prompt construction
- âœ… Anthropic Claude API integration
- âœ… Rate limiting for API calls

### 2. **RL Environment** ([environment.py](src/environment.py))
- âœ… State management (conversation, turn, task)
- âœ… Episode lifecycle (reset, step, done)
- âœ… Multi-agent coordination
- âœ… Natural story completion detection
- âœ… Episode data storage

### 3. **Reward System** ([rewards.py](src/rewards.py))
- âœ… Story quality evaluation (judge LLM)
- âœ… Collaboration scoring
- âœ… Efficiency calculation
- âœ… Weighted reward composition
- âœ… Behavior pattern extraction
- âœ… Learning signal generation

### 4. **Training Loop** ([trainer.py](src/trainer.py))
- âœ… Multi-episode training
- âœ… Metrics tracking
- âœ… Agent learning updates
- âœ… Exploration vs exploitation
- âœ… Checkpoint saving
- âœ… Progress monitoring

### 5. **CLI Interface** ([main.py](main.py))
- âœ… Beautiful Rich-based output
- âœ… Real-time episode display
- âœ… Color-coded agent responses
- âœ… Progress tracking
- âœ… Command-line arguments
- âœ… Configuration management

### 6. **Utilities** ([utils.py](src/utils.py))
- âœ… Training curve plotting
- âœ… Collaboration analysis charts
- âœ… Config file loading
- âœ… Logging setup
- âœ… Learning insights generation
- âœ… Episode formatting

### 7. **Documentation**
- âœ… Comprehensive README
- âœ… Quick start guide
- âœ… Code documentation (docstrings)
- âœ… Type hints throughout
- âœ… Setup instructions
- âœ… Troubleshooting guide

## ğŸ”§ Technical Architecture

### Agent Learning Mechanism

**Prompt-Based Policy Learning** (Simple but Effective):

1. **Episode Execution**: Agents collaborate on a task
2. **Reward Calculation**: Evaluate performance (quality, collaboration, efficiency)
3. **Pattern Extraction**: Identify what worked in high-reward episodes
4. **Prompt Update**: Add successful patterns to agent system prompts
5. **Iteration**: Agents use learned patterns in future episodes

**Example Learning**:
```
Episode 1: Low collaboration â†’ Reward 5.5
Episode 5: Agents build on ideas â†’ Reward 7.8
â†’ Extract pattern: "Reference previous contributions"
â†’ Add to prompt: "Build on teammates' ideas using 'yes, and'"
Episode 10: Consistent collaboration â†’ Reward 8.2
```

### Reward Formula

```
Total Reward = (
    Story Quality Ã— 0.4 +
    Collaboration Ã— 0.4 +
    Efficiency Ã— 0.2
)

Where each component is scored 0-10
```

### Data Flow

```
Config â†’ Create Agents â†’ Create Environment â†’ Create Trainer
                â†“
        Run Episode Loop:
        1. Environment.reset()
        2. For each turn:
           - Agent.act() â†’ Generate response
           - Environment.step() â†’ Update state
        3. RewardCalculator.calculate_rewards()
        4. Trainer.update_agents()
        5. Save metrics
                â†“
        Generate Plots & Summary
```

## ğŸ“Š What Gets Saved

Every training run creates:

```
runs/2025-11-14_HH-MM-SS/
â”œâ”€â”€ episodes.json          # Complete conversation logs
â”œâ”€â”€ rewards.csv           # CSV with all reward data
â”œâ”€â”€ metrics.json          # Summary statistics
â”œâ”€â”€ agent_stats.json      # Agent performance data
â”œâ”€â”€ checkpoint_ep5.json   # Periodic checkpoints
â”œâ”€â”€ checkpoint_ep10.json
â””â”€â”€ plots/
    â”œâ”€â”€ training_curves.png      # 4-panel training visualization
    â””â”€â”€ collaboration_analysis.png  # Collaboration vs quality scatter
```

## ğŸš€ How to Use

### Installation

```bash
# 1. Install dependencies
pip install anthropic pydantic rich numpy matplotlib pyyaml python-dotenv

# Or use requirements.txt
pip install -r requirements.txt

# 2. Set API key
cp .env.example .env
# Edit .env and add: ANTHROPIC_API_KEY=your-key-here

# 3. Run setup verification (optional)
python setup.py
```

### Run Training

```bash
# Quick test (3 episodes)
python main.py --test --verbose

# Full training (20 episodes)
python main.py --episodes 20 --verbose

# Custom configuration
python main.py --config custom_config.yaml --episodes 10

# Show best episode
python main.py --episodes 15 --show-best
```

### View Results

```bash
# Check the latest run directory
ls runs/

# View plots
open runs/2025-11-14_*/plots/training_curves.png

# Read episode data
cat runs/2025-11-14_*/episodes.json | python -m json.tool
```

## ğŸ“ Key Innovations

1. **Prompt Engineering as Policy**: Uses evolving system prompts instead of complex gradients
2. **Collaborative Rewards**: Explicitly measures and rewards teamwork
3. **Judge LLM Evaluation**: Another Claude instance evaluates quality
4. **Real-Time Learning Visibility**: See agents improve during training
5. **Modular Architecture**: Easy to extend to new tasks

## ğŸ”„ Extensibility

### Add New Tasks

Create new environment in `src/environment.py`:

```python
class DebateEnvironment(CollaborativeStoryEnvironment):
    def __init__(self, agents, topic):
        super().__init__(agents)
        self.topic = topic
    # Override methods as needed
```

### Add New Agents

Edit `config.yaml`:

```yaml
agents:
  - role: "Fact Checker"
    goal: "Verify accuracy and credibility"
    color: "magenta"
```

### Custom Rewards

Add to `src/rewards.py`:

```python
def _evaluate_custom_metric(self, episode):
    # Your logic here
    return score
```

## ğŸ“ˆ Expected Performance

### Typical Learning Curve

```
Episodes 1-5:   Avg Reward 5.5-6.5 (Learning basics)
Episodes 6-10:  Avg Reward 6.5-7.5 (Improvement visible)
Episodes 11-15: Avg Reward 7.0-8.0 (Good collaboration)
Episodes 16-20: Avg Reward 7.5-8.5 (Consistent quality)
```

### What Success Looks Like

- âœ… Reward trend line slopes upward
- âœ… Collaboration scores improve faster than other metrics
- âœ… Agents reference each other's contributions
- âœ… Stories have coherent structure
- âœ… Episode lengths stabilize

## ğŸ› Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| API key error | Set `ANTHROPIC_API_KEY` in `.env` |
| Rate limiting | Increase `rate_limit_delay` in config |
| Import errors | Run `pip install -r requirements.txt` |
| No plots | Install matplotlib: `pip install matplotlib` |
| Slow training | Reduce `max_tokens` or `max_turns` |

## ğŸ”¬ Testing

```bash
# Run basic tests
python tests/test_basic.py

# Expected output: All tests pass (except if dependencies not installed)
```

Tests cover:
- Agent memory functionality
- State management
- Episode tracking
- Reward calculation
- Config validation

## ğŸ“ Code Quality

- âœ… **Type hints** throughout all code
- âœ… **Docstrings** for all major functions
- âœ… **Logging** with appropriate levels
- âœ… **Error handling** for API calls
- âœ… **Modular design** with clear separation
- âœ… **Configuration-driven** behavior

## ğŸ¯ Future Enhancements

Potential additions (not implemented):

1. **More Tasks**: Code review, planning, brainstorming
2. **Human-in-the-Loop**: Allow user feedback during training
3. **Multi-Task Learning**: Train on multiple tasks
4. **Advanced RL**: Implement PPO or other algorithms
5. **Agent Communication**: Let agents "talk" about strategy
6. **Visualization Dashboard**: Web-based real-time monitoring

## ğŸ’» Technology Stack

- **LLM**: Anthropic Claude (Sonnet 3.5)
- **CLI**: Rich library for beautiful terminal output
- **Config**: PyYAML for configuration
- **Plotting**: Matplotlib for charts
- **Data**: Pydantic for validation, JSON for storage
- **Environment**: python-dotenv for secrets

## ğŸ“– Documentation Files

1. **README.md**: Complete user guide (350+ lines)
2. **QUICKSTART.md**: 5-minute setup guide
3. **PROJECT_SUMMARY.md**: This technical overview
4. **Code Docstrings**: Inline documentation
5. **Config Comments**: Inline configuration docs

## âœ¨ What Makes This Special

1. **Complete Working System**: Not a toy - a real RL environment
2. **Visible Learning**: Watch agents improve in real-time
3. **Production Quality**: Error handling, logging, testing
4. **Extensible**: Clean architecture for adding features
5. **Educational**: Learn both RL and LLM concepts
6. **Beautiful UX**: Rich CLI with colors and formatting

## ğŸ¬ Ready to Run

This is a **complete, working prototype**. Everything you need:

âœ… All code files
âœ… Configuration
âœ… Documentation
âœ… Tests
âœ… Examples
âœ… Setup scripts

Just add your API key and run!

## ğŸ¤ Next Steps

1. **Install dependencies**: `pip install -r requirements.txt`
2. **Set API key**: Edit `.env` file
3. **Run test**: `python main.py --test --verbose`
4. **Read results**: Check `runs/` directory
5. **Experiment**: Modify `config.yaml`
6. **Extend**: Add new tasks or agents

---

**Built with Claude** ğŸ­
A complete Multi-Agent RL system for LLM collaboration research.

For detailed usage, see [README.md](README.md)
For quick start, see [QUICKSTART.md](QUICKSTART.md)
