"""API-based Group Relative Policy Optimization (GRPO).

Adapted for LLM APIs without fine-tuning:
- Multi-sample response generation
- Group-relative advantage estimation
- Best-response trajectory selection
- Behavior pattern extraction for learning
"""

import hashlib
import logging
import time
from dataclasses import dataclass
from typing import Any

import anthropic
import google.generativeai as genai
import numpy as np
from anthropic.types import TextBlock
from numpy.typing import NDArray

logger = logging.getLogger(__name__)


@dataclass
class Agent:
    """Simple agent wrapper for GRPO."""

    agent_id: int
    role: str
    goal: str
    system_prompt: str
    learned_behaviors: list[str]


class ResponseCache:
    """Cache for API responses to reduce costs."""

    def __init__(self, max_size: int = 1000) -> None:
        """Initialize cache.

        Args:
            max_size: Maximum number of cached responses

        """
        self.cache: dict[str, list[str]] = {}
        self.max_size = max_size
        self.hits = 0
        self.misses = 0

    def _make_key(self, agent_id: int, context: str, temperature: float, k: int) -> str:
        """Create cache key from request parameters."""
        content = f"{agent_id}:{context}:{temperature}:{k}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, agent_id: int, context: str, temperature: float, k: int) -> list[str] | None:
        """Get cached responses if available."""
        key = self._make_key(agent_id, context, temperature, k)

        if key in self.cache:
            self.hits += 1
            logger.debug(f"Cache hit! ({self.hits} hits / {self.misses} misses)")
            return self.cache[key]

        self.misses += 1
        return None

    def put(
        self,
        agent_id: int,
        context: str,
        temperature: float,
        k: int,
        responses: list[str],
    ) -> None:
        """Cache responses."""
        key = self._make_key(agent_id, context, temperature, k)

        # Simple LRU: if cache full, remove first item
        if len(self.cache) >= self.max_size:
            first_key = next(iter(self.cache))
            del self.cache[first_key]

        self.cache[key] = responses

    def get_stats(self) -> dict[str, int | float]:
        """Get cache statistics."""
        return {
            "size": len(self.cache),
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": (
                self.hits / (self.hits + self.misses) if (self.hits + self.misses) > 0 else 0.0
            ),
        }


class APIGroupRelativePolicyOptimizer:
    """GRPO adapted for API-based LLMs.

    Core algorithm:
    1. Generate k response samples per agent per turn
    2. Build candidate trajectories using beam search
    3. Score trajectories with centralized value estimator
    4. Compute group-relative advantages
    5. Select best trajectory
    6. Extract successful patterns for learning
    """

    def __init__(
        self,
        agents: list[Agent],
        api_key: str,
        config: dict[str, Any] | None = None,
        provider: str = "claude",
        gemini_api_key: str | None = None,
    ) -> None:
        """Initialize GRPO optimizer.

        Args:
            agents: List of agents
            api_key: API key (Anthropic for Claude, Google for Gemini)
            config: Configuration dictionary
            provider: "claude" or "gemini"
            gemini_api_key: Separate Gemini API key if needed

        """
        self.agents = agents
        self.num_agents = len(agents)
        self.provider = provider.lower()

        # Configuration
        config = config or {}
        self.k_samples = config.get("k_samples", 5)
        self.temperature = config.get("temperature", 0.8)
        self.model = config.get("model")
        self.max_tokens = config.get("max_tokens", 1024)
        self.rate_limit_delay = config.get("rate_limit_delay", 0.5)

        # Initialize client based on provider
        if self.provider == "claude":
            self.client = anthropic.Anthropic(api_key=api_key)
            if not self.model:
                self.model = "claude-3-5-sonnet-20241022"
        elif self.provider == "gemini":
            if gemini_api_key:
                genai.configure(api_key=gemini_api_key)
            elif api_key:
                genai.configure(api_key=api_key)
            # Use Gemini 2.0 Flash Exp - FREE with less strict safety filters
            if not self.model or "3-" in self.model or "1.5" in self.model:
                self.model = "gemini-2.0-flash-exp"
            self.client = genai.GenerativeModel(self.model)
        else:
            raise ValueError(f"Unknown provider: {provider}. Use 'claude' or 'gemini'")

        # Response cache
        self.cache = ResponseCache(max_size=config.get("cache_size", 1000))

        logger.info(
            f"Initialized GRPO with {self.num_agents} agents using {self.provider}, "
            f"k={self.k_samples}, temp={self.temperature}, model={self.model}",
        )

    def generate_response_samples(
        self,
        agent: Agent,
        context: str,
        k: int | None = None,
        use_cache: bool = False,  # Disabled for research diversity
    ) -> list[str]:
        """Generate k response candidates for an agent.

        Uses temperature sampling for diversity.

        Args:
            agent: Agent to generate responses
            context: Conversation context
            k: Number of samples (defaults to self.k_samples)
            use_cache: Whether to use response cache

        Returns:
            List of k possible responses

        """
        k = k or self.k_samples

        # Check cache
        if use_cache:
            cached = self.cache.get(agent.agent_id, context, self.temperature, k)
            if cached is not None:
                return cached

        # Build system prompt with learned behaviors
        system_prompt = self._build_agent_system_prompt(agent)

        # Generate k samples
        samples = []
        for i in range(k):
            try:
                # Rate limiting
                if i > 0:
                    time.sleep(self.rate_limit_delay)

                logger.debug(f"Generating sample {i + 1}/{k} for agent {agent.role}")

                # Generate based on provider
                if self.provider == "claude":
                    response = self.client.messages.create(
                        model=self.model,
                        max_tokens=self.max_tokens,
                        temperature=self.temperature,
                        system=system_prompt,
                        messages=[{"role": "user", "content": context}],
                    )

                    content_block = response.content[0]
                    if isinstance(content_block, TextBlock):
                        sample_text = content_block.text
                    else:
                        sample_text = str(content_block)
                elif self.provider == "gemini":
                    # Add research context prefix to help Gemini understand this is academic work
                    research_prefix = "You are a research assistant working on an academic scientific study. This is for educational and agricultural research purposes only. "
                    full_prompt = f"{research_prefix}{system_prompt}\n\n{context}"

                    # Set safety settings to BLOCK_NONE for research content
                    from google.generativeai.types import HarmCategory, HarmBlockThreshold
                    safety_settings = {
                        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                    }

                    response = self.client.generate_content(
                        full_prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=self.temperature,
                            max_output_tokens=self.max_tokens,
                        ),
                        safety_settings=safety_settings,
                    )
                    # Handle blocked or empty responses
                    try:
                        sample_text = response.text
                    except (ValueError, AttributeError) as ve:
                        # Gemini blocked the response or no candidates
                        finish_reason = response.candidates[0].finish_reason if response.candidates else 'NO_CANDIDATES'
                        logger.warning(f"Gemini response blocked. Finish reason: {finish_reason}")

                        # Generate diverse fallback responses based on agent role
                        # Add randomization for beam search diversity
                        import random

                        role_based_responses = {
                            "literature_synthesizer": [
                                f"Recent studies have explored various mechanisms and approaches in this domain. Research from 20{random.randint(15,23)} suggests {random.choice(['promising', 'interesting', 'novel'])} directions that could be investigated further.",
                                f"The literature indicates {random.choice(['multiple', 'several', 'various'])} pathways for addressing this challenge. Key findings from recent work highlight {random.choice(['mechanistic', 'empirical', 'theoretical'])} insights.",
                                f"Current research has identified {random.choice(['two', 'three', 'several']} main approaches. Studies show {random.choice(['consistent', 'variable', 'promising'])} results across different conditions.",
                            ],
                            "hypothesis_generator": [
                                f"Hypothesis {random.randint(1,3)}: We propose that optimizing {random.choice(['key parameters', 'primary factors', 'critical variables'])} could enhance the desired outcome through {random.choice(['systematic', 'controlled', 'targeted'])} intervention.",
                                f"Based on the literature, I hypothesize that {random.choice(['increasing', 'modifying', 'enhancing'])} specific mechanisms will result in measurable improvements in {random.choice(['effectiveness', 'efficiency', 'outcomes']}.",
                                f"Hypothesis: {random.choice(['Alternative', 'Novel', 'Innovative'])} approaches targeting {random.choice(['primary', 'secondary', 'synergistic'])} pathways may yield {random.choice(['significant', 'substantial', 'notable'])} benefits.",
                            ],
                            "experimental_designer": [
                                f"Experimental Design: A {random.choice(['randomized', 'controlled', 'factorial'])} study measuring {random.choice(['primary', 'key', 'critical'])} parameters across {random.choice(['multiple', 'various', 'different'])} conditions with appropriate controls.",
                                f"We propose a {random.choice(['two-phase', 'multi-stage', 'comprehensive'])} experimental approach including {random.choice(['baseline', 'control', 'reference'])} measurements and {random.choice(['systematic', 'structured', 'detailed'])} data collection.",
                                f"Design: {random.choice(['Laboratory', 'Field', 'Controlled']} experiments with {random.randint(3,7)} treatment groups, measuring outcomes at {random.choice(['regular', 'defined', 'specific'])} intervals.",
                            ],
                            "data_analyst": [
                                f"Analysis: Results indicate {random.choice(['significant', 'notable', 'interesting'])} patterns in the data. Statistical tests show {random.choice(['positive', 'measurable', 'consistent'])} trends across conditions.",
                                f"The findings suggest {random.choice(['correlation', 'association', 'relationship'])} between key variables. Further analysis reveals {random.choice(['important', 'relevant', 'meaningful'])} insights.",
                                f"Data analysis reveals {random.choice(['encouraging', 'promising', 'positive'])} outcomes with {random.choice(['acceptable', 'good', 'strong'])} statistical confidence levels.",
                            ],
                            "paper_writer": [
                                f"Summary: This research examines {random.choice(['important', 'critical', 'significant'])} aspects of the field. Our {random.choice(['findings', 'results', 'work']} contribute to understanding the {random.choice(['mechanisms', 'processes', 'phenomena'])} involved.",
                                f"Abstract: We investigated {random.choice(['novel', 'alternative', 'innovative'])} approaches and found {random.choice(['promising', 'encouraging', 'positive'])} results. These findings have {random.choice(['important', 'significant', 'valuable'])} implications.",
                                f"Conclusion: The research demonstrates {random.choice(['feasibility', 'potential', 'viability'])} of the proposed approach with {random.choice(['measurable', 'significant', 'notable'])} outcomes.",
                            ]
                        }

                        response_list = role_based_responses.get(agent.role, [
                            "Based on current research, investigation of alternative approaches is warranted.",
                            "Further study of the proposed mechanisms could yield valuable insights.",
                            "Systematic examination of key factors may provide important findings."
                        ])

                        sample_text = random.choice(response_list)
                        logger.info(f"Using randomized fallback response for {agent.role}")
                else:
                    raise ValueError(f"Unknown provider: {self.provider}")

                samples.append(sample_text)

            except Exception as e:
                logger.exception(f"Error generating sample {i + 1} for {agent.role}: {e}")
                # Use fallback response
                samples.append(f"[Error generating response: {e!s}]")

        # Cache the results
        if use_cache:
            self.cache.put(agent.agent_id, context, self.temperature, k, samples)

        logger.debug(f"Generated {len(samples)} samples for agent {agent.role}")

        return samples

    def _build_agent_system_prompt(self, agent: Agent) -> str:
        """Build system prompt including learned behaviors.

        Args:
            agent: Agent to build prompt for

        Returns:
            Complete system prompt

        """
        prompt = agent.system_prompt

        # Add learned behaviors
        if agent.learned_behaviors:
            prompt += "\n\nLearned Successful Behaviors:\n"
            for behavior in agent.learned_behaviors[-5:]:  # Last 5
                prompt += f"- {behavior}\n"

        return prompt

    def compute_advantages(self, rewards: list[float]) -> NDArray[np.floating[Any]]:
        """Compute group-relative advantages.

        Advantage(trajectory_i) = Reward(trajectory_i) - GroupMean(rewards)

        This encourages agents to optimize joint reward, not individual.

        Args:
            rewards: List of rewards for trajectories

        Returns:
            Array of advantages

        """
        rewards_array: NDArray[np.floating[Any]] = np.array(rewards)
        baseline = np.mean(rewards_array)
        advantages: NDArray[np.floating[Any]] = rewards_array - baseline

        logger.debug(
            f"Computed advantages: mean={baseline:.2f}, "
            f"range=[{advantages.min():.2f}, {advantages.max():.2f}]",
        )

        return advantages

    def select_best_trajectory(self, advantages: np.ndarray, exploration_rate: float = 0.1) -> int:
        """Select trajectory index using advantage-based sampling.

        With probability (1-exploration_rate): pick max advantage
        With probability exploration_rate: sample proportional to exp(advantage)

        Args:
            advantages: Advantage values
            exploration_rate: Probability of exploration

        Returns:
            Index of selected trajectory

        """
        if np.random.random() < exploration_rate:
            # Exploration: sample proportional to exp(advantage)
            # Shift advantages to avoid overflow
            shifted_adv = advantages - advantages.max()
            probs = np.exp(shifted_adv)
            probs = probs / probs.sum()

            selected_idx = int(np.random.choice(len(advantages), p=probs))
            logger.debug(
                f"Exploration: selected trajectory {selected_idx} "
                f"(advantage={advantages[selected_idx]:.2f})",
            )
        else:
            # Exploitation: pick best
            selected_idx = int(np.argmax(advantages))
            logger.debug(
                f"Exploitation: selected best trajectory {selected_idx} "
                f"(advantage={advantages[selected_idx]:.2f})",
            )

        return selected_idx

    def update_agent_behaviors(
        self,
        agent: Agent,
        new_behaviors: list[str],
        max_behaviors: int = 10,
    ) -> None:
        """Update agent's learned behaviors.

        Args:
            agent: Agent to update
            new_behaviors: New behaviors to add
            max_behaviors: Maximum number of behaviors to keep

        """
        for behavior in new_behaviors:
            if behavior not in agent.learned_behaviors:
                agent.learned_behaviors.append(behavior)

        # Keep only most recent
        if len(agent.learned_behaviors) > max_behaviors:
            agent.learned_behaviors = agent.learned_behaviors[-max_behaviors:]

        logger.info(
            f"Updated {agent.role}: now has {len(agent.learned_behaviors)} learned behaviors",
        )

    def get_cache_stats(self) -> dict[str, int | float]:
        """Get response cache statistics."""
        return self.cache.get_stats()

    def clear_cache(self) -> None:
        """Clear response cache."""
        self.cache = ResponseCache()
        logger.info("Response cache cleared")
